---
title: 'Applied Data Science Using R - INF 2167H - Assignment #2'
author: "Faria Khandaker"
header-includes:
- \usepackage{float}
- \floatplacement{figure}{H}
output:
  word_document: default
  pdf_document:
    fig_cap: yes
    keep_tex: yes
    fig_width: 5
    fig_height: 3
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

Use RStudio for this assignment. 
Edit the file `assignment-02.Rmd` and insert your R code where wherever you see the string "INSERT YOUR ANSWER HERE"

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. 


## Sample Question and Solution

Use `seq()` to create the vector $(2,4,6,\ldots,20)$.

```{r}
#Insert your code here.
seq(2,20,by = 2)
```
```{r Libraries, message=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
knitr::opts_chunk$set(fig.pos = "!H")
library(tidyverse)
library(dplyr)
library(lmtest)
library(ggplot2)
library(rcompanion)
library(MASS)
library(corrplot)

```

## Question 1  Visualizations and missing variables
#### 1. Read the csv files "USDA_Macronutrients.csv" and "USDA_Micronutrients.csv" attached with the assignment. 

```{r}
micro<-read.csv(file.choose(), header = T)
macro<-read.csv(file.choose(), header = T)
```

#### 2. Merge the two data frames using the variable "ID". Name the Merged Data Frame "USDA". You should get a dataframe of 7057 cases and 15 variables. 
Hint: explore (merge) function

```{r}
usda<-merge(micro,macro, by.micro=micro$ID)
```

#### 3. Delete the commas in the Sodium and Potasium records and display the first 21 lines of these two columns after removing the commas. Assign Sodium and Potasium as numeric data types. Check the datatypes of the attributes.

```{r}
usda$Sodium<-gsub(",","",usda$Sodium)
usda$Potassium<-gsub(",","",usda$Potassium)
head(usda$Sodium,21)
head(usda$Potassium,21)
usda$Sodium<-as.numeric(usda$Sodium)
usda$Potassium<-as.numeric(usda$Potassium)
```

#### 4. Are there missing values in USDA? If yes, how many?

There are 8827 missing values in the USDA Dataset.

```{r}
sum(is.na(usda))

```

#### 5. Remove records (rows) with missing values in more than 4 attributes (columns). Keep using the same name USDA. How many records remain in the new USDA data frame? 

After removing the rows containing more than 4 missing values, the new dataset has 6949 missing values.

```{r}
x=nrow(usda)
y<-0
for (i in 1:x){
  if (sum(is.na(usda[i,]))>4){
    usda<- usda[-c(i),]
  }
}
nrow(usda)
```

#### 6. For records with missing values for Sugar, Vitamin E and Vitamin D, replace missing values with mean value for the respective variable. 

```{r}
msug<-round(mean(na.omit(usda$Sugar)),digits=2)
mvite<-round(mean(na.omit(usda$VitaminE)),digits=2)
mvitd<-round(mean(na.omit(usda$VitaminD)),digits=2)

usda$Sugar<-replace(usda$Sugar, is.na(usda$Sugar), msug)

usda$VitaminE<-replace(usda$VitaminE, is.na(usda$VitaminE), mvite)

usda$VitaminD<-replace(usda$VitaminD, is.na(usda$VitaminD), mvitd)

```

#### 7. With a single line of code, remove all remaining records with missing values. Name the new dataframe "USDAclean" and continue the rest of the exercises using this dataframe. How many records are in the USDAclean  data frame? 

The USDAclean dataset has 6310 records.

```{r}
USDAclean<-na.omit(usda)
nrow(USDAclean)
```

#### 8. Which food has the highest sodium level in USDAclean? 

Table salt has the highest sodium level.

```{r}
USDAclean[which(USDAclean$Sodium==max(USDAclean$Sodium)),"Description"]
```

#### 9. Create a histogram of Vitamin C distribution in foods from USDAclean, with a limit of 0 to 100 on the x-axis and breaks of 100. 

This is the histogram. 

```{r}
hist(USDAclean$VitaminC, breaks = 100, xlim=range(0,100))
```



#### 10. Create a boxplot to illustrate the distribution of values for TotalFat, Protein and Carbohydrate in USDAclean. 

This is the boxplot

```{r}
boxplot(USDAclean$TotalFat, USDAclean$Protein,USDAclean$Carbohydrate, names = c("TotalFat", "Protein", "Carbohydrate"), ylim=c(0,100))
```

#### 11. Create a scatterplot to illustrate the relationship between a food's TotalFat content and its calorie content in USDAclean.

This is the scatterplot.

```{r}
plot(x=USDAclean$TotalFat, y=USDAclean$Calorie)
```

## Question 2 Skewness and transformations
## Question 2 uses the dataset Carseats in library ISLR
#### 1. Install the ISLR package if it's not already installed and read the Carseats dataset
```{r message=FALSE, warning=FALSE}
#install.packages("ISLR")
library(ISLR)
data("Carseats")
```

#### 2. Draw a normal histogram of the Advertising column, is the Advertising distribution skewed? If yes, to which direction? Run an appropriate test to confirm your answer about skewness. Interpret the result of the test.

The distribution is right skewed

```{r}
plotNormalHistogram(Carseats$Advertising)
```

The Shapiro0-Wilkes test was run to test the normality assumption. The p-value is less than 0.05 which means the normality assumption is violated(null hypothesis is rejected). This confirms that the values in the Advertising column are skewed.

```{r}
shapiro.test(Carseats$Advertising)
```

#### 3. If the Advertising variable in the previous question is skewed, find an adequate transformation to bring it close to normality. What is the best lambda you get? Based on the value of lambda, would transformation make a big difference?

I tried to apply the log function to reduce skewness but it seemed to make the values become left skewed. 
```{r}
advlog <-log(Carseats$Advertising)
hist(advlog)
```
Then I moved on to trying Tukey transformation
The value of lambda is 0.8 and the distribution is less skewed after its application. A lambda of 0.8 is very close to 1 and this indicates that performing any sort of transformation on this dataset will not help improve skewness by much.

```{r}
advtuk <- transformTukey(Carseats$Advertising)
```
I performed the square-root function but this did not improve the skewness by much. Square-rooting had a better effect than cube root as the lambda for square-root is 0.5 and the lambda for cube-root is 0.3 and 0.5 is closer to the 0.8 lambda suggested by the Tukey transformation function.
```{r}
#square-root
tvalues<-sqrt(Carseats$Advertising)
plotNormalHistogram(tvalues)

```
```{r}
#cube-root
T_cub <- sign(Carseats$Advertising) * abs(Carseats$Advertising)^(1/3)
plotNormalHistogram(T_cub)
```

## Question 3   
The following question make use of data that is provided by the `mosaic` package.  (install mosaic package and load KidsFeet using data(KidsFeet) ). 
```{r message=FALSE, warning=FALSE}
#install.packages("mosaic")
library(mosaic)
data("KidsFeet")
```

#### 1. Display the first 6 rows of the KidsFeet dataset.

```{r}
head(KidsFeet)
```

#### 2. Display the type of each column of the KidsFeet dataset, use only one function in R to do so. Do not use (str)

```{r}
sapply(KidsFeet, class)
```

#### 3. Plot a histogram of the `width` variable.

```{r}
hist(KidsFeet$width)
```

#### 4. Draw a normal histogram to show whether width is normally distributedor not. Use an appropriate test to check if width is normally distributed. Interpret your result. 

The p-value of width, according to the Shapiro-Wilkes test, is 0.2811,which is higher than 0.05. This means we cannot reject the assumption of normality (null hypothesis is not rejected). This confirms that 'width' is normally distributed

```{r}
plotNormalHistogram(KidsFeet$width)
shapiro.test(KidsFeet$width)
```

#### 5. Create a boxplot which shows the distribution of `width` in each `birth month`. Use different colors for each `birth month`.

```{r}
ggplot(KidsFeet,aes(x=as.factor(birthmonth),y=width, fill=as.factor(birthmonth)))+ geom_boxplot()

```

#### 6. Create one scatter plot matrix of the numeric variables (length, width) within the KidsFeet dataset.
(Hint investigate pairs())

```{r}
pairs(~length+width,data=KidsFeet)
```


## Question 4    Model Assumptions
This question makes use of package "datarium". Load marketing dataset from the package
```{r message=FALSE, warning=FALSE}
#install.packages("datarium")
library(datarium)
data("marketing")
```

#### 1. build a model to predict sales on the basis of advertising budget spent in youtube. Show the results of the model and interpret the coefficient of youtube (beta value and significance)

Investment in YouTube has significant impacts on sales. The p-value of YouTube is less than 0.05 which means it significantly impacts sales. The estimated coefficient(BETA) of youtube is 0.0475. The equation of the regression line becomes y=0.0475x+8.439. This means for every 1 unit increase in YouTube investment, sales goes up by 0.0475 units. 

```{r}
sales_model<- lm(sales~youtube, data=marketing)
summary(sales_model)
```

#### 2. Plot the appropriate model plot to check for linearity, which plot tells about linearity? Interpret the appropriate plot

Looking at the residuals vs fitted plot, the linearity assumption appears to hold as the red trend line appears to closely follow the horizontal dotted line except in a few places. The red line does not have a strong curve and therefore we can assume that the linearity assumption holds. 

```{r}
plot(sales_model, which=c(1))
```

#### 3. Use two methods to check the assumption of constant variance (homoscedasticity) of the model. Interpret your results.

The first method used to check for homoscedasticity is the scale-location plot. The redline on the graph is not horizontal and follows an upward trend. This indicates non-constant variance.
```{r}
plot(sales_model, which=c(3))
```
The second method used to check for homoscedasticity is the Breusch-Pagan test. The p-value found was found to be less than 0.05. This mean the null hypothesis of constant variance is rejected. The variance in the model is non-constant.
```{r}
bptest(sales_model)
```

#### 4. Use three methods to check the assumption of normality of the model, write one sentence after each method to write the name of the method and interpret its result.

There are three methods to check for normality: Histograms, Q-Q plots, and Shapiro-Wilkes Test. 

Method 1: Histogram: the histogram displayed appears to have a normal distribution judging by how the bars are distributed around the center
```{r}
hist(resid(sales_model))
```
Method 2: QQplot: The Residuals that are plotted appear follow the line pretty closely. This indicates that the error are very close to a normal distribution

```{r}
qqnorm(resid(sales_model), main = "Normal Q-Q Plot, SalesModel", col = "darkgrey")
qqline(resid(sales_model), col = "dodgerblue", lwd = 2)
```
Method 3: Shapiro-Wilkes test: a p-value of 0.2133 is greater than 0.05. This means the null hypothesis is not rejected and so the errors are regarded as following a normal distribution
```{r}
shapiro.test(resid(sales_model))
```

#### 5. Are there any outliers or influential points in the model? Draw the plot that makes you decide? Interpret your answer

According to the residuals vs leverage plot, there are three points which can be considered outliers: 26, 179, and 36. These points are outliers because they lie close to 3 standard deviations to the left of the distribution (as indicated by the negative side). The points are not influential points as there aren't any dotted Cook's distance lines given to show which points are beyond that distance.

```{r}
plot(sales_model,which=c(5))
```

#### 6. Fix the homoscedasticity of the model through creating a new linear model using an appropriate transformation. Draw the homoscedasticity plot of the new model showing it is fixed

The red line in the Scale-Location Plot,which is used to show homoscedasticity, is much more straight after applying the log transformation. The errors follow a linear pattern.

```{r}
loggedsales<- log(marketing$sales)
marketing<- marketing %>%mutate(LogSale=loggedsales)

sales_model2<-lm(LogSale~youtube, data=marketing)
plot(sales_model2, which=c(3))
```

## Question 5 -  Regression Analysis
This question makes use of package "plm",  load Crime dataset from this package

```{r message=FALSE, warning=FALSE}
#install.packages("plm")
library(plm)
data(Crime)
```

#### 1. Display the first 10 rows of crime and the structure of all the variables.

```{r}
head(Crime, 10)
str(Crime)
```

#### 2. Calculate the mean,variance and standard deviation of tax revenue per capita (taxpc) by omitting the missing values, if any.

The mean, variance and standard deviation of the 'tax revenue per capita' variable are 30.24, 131.21, and 11.46 respectively.
```{r}
taxmean<-mean(na.omit(Crime$taxpc))
taxvariance<-var(na.omit(Crime$taxpc))
taxsd<-sd(na.omit(Crime$taxpc))
taxmean
taxvariance
taxsd
```

#### 3. Use `density` and `smsa` to predict tax per capita and build a multivariate linear regression model, display a summary of your model indicating Residuals, Coefficients..etc. Interpret adjusted R squared, the intercept, coefficients and significance of the predictors in your model?

  The Adjusted R squared is 0.063. This means only 6 percent of the variance can be explained by the variables 'density' and 'smsa' and 94% of the variation can be explained by other variables not considered by the model. 
  
  The intercept for this model is 29.5615 which means if all other variables are at zero, the tax revenue per capita would be 29.5615 units. 
  
  The coefficient for 'density' is -0.2345. This means that keeping all other variables constant, tax per capita decreases by 0.2345 for every one unit increase in density. 

  The coefficient for 'smsayes'(smsa is a categorical variable with values yes or no filling the columns) is 11.2808. Compared to the reference category(smsano), smsayes impacts tax per capita by 11.2808 units. 
  
The variable of 'density' is not considered statistically significant as the p-value is greater than 0.05. The p-value for 'smsayes' is less than 0.05 but smsa is a categorical variable and the significance for it can only be determined by performing a chi-square test.

```{r}
taxmodel<-lm(taxpc~density+smsa, data=Crime)
summary(taxmodel)
```


#### 4. Based on the output of your model, write the equations based on the intercept and factors of `smsa` when `density` is set to 2.4, and compare the result with `predict()` function. Hint: Explore `predict()` function
##### Hint: there will be two equations, one for smsa (no) and one for smsa(yes)
##### Hint: create a new dataframe on which to run the prediction having density of 2.4 and smsa a vector of no and yes

The results of the predict function are very closely in line with the results of running the equations separately.

```{r}
#Eqn for smsayes(1)
smyes <- -0.2345*(2.4)+11.2808*(1)+29.5615
#Eqn for smsano(0)
smno <- -0.2345*(2.4)+11.2808*(0)+29.5615
new.case<-data.frame(density=c(2.4),smsa=c("yes","no"))
taxpred<-predict(taxmodel,new.case)
smyes
smno
taxpred
```

#### 5. Which independent variable has the strongest positive predictive power in the model? (Hint: Look at the coefitients calculated for each independent variable)

The independent variable 'crmrte', which stands for "crimes committed per person" has the strongest positive predictive power in the model
```{r}
full <- lm(taxpc~.,data=Crime)
stepB <- stepAIC(full, direction= "backward", trace=FALSE)
summary(stepB)
```

#### 6. Find Pearson correlation between tax per capita and density. Interpret the result with a sentence.

  The variables 'density' and 'taxpc' have a correlation coefficient of 0.1998. A strong correlation relationship does not exist between these two variables. The population density of an area is not strongly correlated to the tax revenue per capita.

```{r}
numdat<- Crime[,c(9,10)]
correlationMatrix <- cor(numdat, method = "pearson") 
correlationMatrix 


```

#### 7. Write the correlation matrix of the variables: avgsen, polpc, density, taxpc. Hint: Explore the variables using ?Crime. Comment on the result with a sentence.

There aren't any variables that are strongly correlated with each other. Most correlations are below 10 percent. the correlations that show the strongest relationships are tax revenue per capital to police per capita with a correlation coefficient of 0.1083 and tax revenue per capital to density which is 0.1998. These are the highest coefficients but the relationship is considered weak.

```{r}
numdat2<- Crime[,c(7,8,9,10)]
correlationMatrix2 <- cor(numdat2, method = "pearson") 
correlationMatrix2
```

#### 8. Draw the correlation plot of the variables: avgsen, polpc, density, taxpc. 
This is the Correlation Plot

```{r warning=FALSE}

pal <- colorRampPalette(c("red", "blue", "green"))

corrplot(correlationMatrix2, method="number", col=pal(200),   
         type="upper", order="hclust", 
         tl.col="black", tl.srt=45, tl.cex= 0.9, diag=TRUE)  
```

## Question 6  - Visualization
#### 1. Generate normally distributed random numbers for three categories:  A (n = 200, mean = 100, sd = 20), B (n = 200, mean = 120, sd = 20), and C (n = 200, mean = 80, sd = 20)
Hint: explore rnorm

```{r}
A <- rnorm(n = 200, mean = 100, sd = 20)
B <- rnorm(n = 200, mean = 120, sd = 20)
C<- rnorm(n = 200, mean = 80, sd = 20)
```

#### 2. Combine all the three categories in one dataframe so that A is the first column of the dataframe, B is the second column, and C is the third column.  Then generate a density plot of the data colored by category.
hint: explore the function (melt) from the library reshape2 on the created dataframe then use geom_density in ggplot2

```{r}
library(reshape2)
randomdataset<-data.frame(A,B,C)
randomdataset2<-melt(randomdataset)
ggplot(randomdataset2,aes(x=value, fill=variable))+geom_density(alpha=0.5)
```

END of Assignment #2.